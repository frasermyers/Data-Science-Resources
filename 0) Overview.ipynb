{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data Science Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Science Process\n",
    "\n",
    "1) Setting the research goal\n",
    "\n",
    "2) Retrieving data\n",
    "\n",
    "3) Data preparation \n",
    "\n",
    "4) Data exploration \n",
    "\n",
    "5) Data modelling\n",
    "\n",
    "6) Presentation and automation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setting the Research Goal   \n",
    "\n",
    "Aim to get formal __agreement__ on __deliverables__ through a _project charter_, which may inclue:\n",
    "    \n",
    "- Statement of research __goal__\n",
    "\n",
    "- Broader project missing and __context__\n",
    "\n",
    "- Required __resources__ and __data__\n",
    "\n",
    "- How __analysis__ will be performed \n",
    "\n",
    "- Proof that it's an __achievable__ project\n",
    "\n",
    "- Measure of __success__\n",
    "\n",
    "- Formal __deliverables__ (e.g. project report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Identify and Retrieve Data\n",
    "\n",
    "This is the first time you inspec the data in the data science process. Most of the errors here are easy to spot. \n",
    "\n",
    "Focus on:\n",
    "\n",
    "- If the data is equal to the data in the __source document__ \n",
    "\n",
    "- If you have the right __data types__\n",
    "\n",
    "Stop when you have enough evidence that the data is similar to the data you find in the source document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Preparation \n",
    "\n",
    "Here, do a more elaborate check of the data. The errors here should also be present in the __source document__.\n",
    "\n",
    "Focus is on the __content__ of the variables:\n",
    "\n",
    "- Typos: USQ to USA\n",
    "\n",
    "- Other data entry errors\n",
    "\n",
    "- Missing values \n",
    "\n",
    "- Inconsistencies: \"F\" or \"Female\"\n",
    "\n",
    "- Transformation (e.g. total GDP to per capita GDP, categorical data to numerical\n",
    "\n",
    "This phase often consues a lot of time, but it's vital.\n",
    "\n",
    "__Garbage in, garbage out.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Exploration \n",
    "\n",
    "Now, take a deep dive into the data using __graphical techniques__.\n",
    "\n",
    "The aim is to:\n",
    "\n",
    "- gain an __understanding__ of __each feature__ \n",
    "\n",
    "- gain an __understanding__ of the __interactions__ between features\n",
    "\n",
    "- Include descriptive statistics: mean, median, mode, sd. \n",
    "\n",
    "This phase is about __exploring__ data, so keeping your mind open and your eyes peeled is important.\n",
    "\n",
    "The goal isn't to cleanse the data, but it's common that you'll still discover anomalies you missed before, forcing you to take a step back and fix them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Data Modelling \n",
    "\n",
    "The modelling phase consists of __four steps__:\n",
    "\n",
    "- __Feature engineering__ and __model selection__\n",
    "\n",
    "- __Training__ the model\n",
    "\n",
    "- Model __validation and selection__\n",
    "\n",
    "- Applying the trained model to __unseen data__\n",
    "\n",
    "Before you find a good model, you'll probably __iterate__ among the first three steps. \n",
    "\n",
    "The __last step__ isn't always present because sometimes the goal isn't prediction but explanation (__root cause analysis__).\n",
    "\n",
    "- For example, you might want to find out the __causes of species' extinctions__ but not necessarily predict which one is next in line to leave our planet.\n",
    "\n",
    "### Engineering Features and Selecting a Model\n",
    "\n",
    "With engineering features, you must come up with and creat __possible predictors__ for the model.\n",
    "\n",
    "This is one of the most important steps in the process because a model __recombines these features__ to __achieve its predictions__. \n",
    "\n",
    "Often you may need to __consult an expert__ or __appropriate literature__ to come up with __meaningful features__.\n",
    "\n",
    "### Training Your Model\n",
    "\n",
    "In this phase, you __present your model with data__ from which it can learn.\n",
    "\n",
    "The __most common modelling techniques__ have __industry-ready implementations__ in almost every programming language, including __Python__.\n",
    "\n",
    "These enable you to __train__ your models by __executing a few lines of code__. \n",
    "\n",
    "Once a model is trained, it's time to test whether it can be extrapolated to reality (__model validation__).\n",
    "\n",
    "### Validating a Model\n",
    "\n",
    "Data science has many modelling techniques, and the question is __which one is the right one to use__?\n",
    "\n",
    "A good model has two properties:\n",
    "\n",
    "- It has __good predictive power__\n",
    "\n",
    "- It __generalizes well__ to data it hasn't seen\n",
    "\n",
    "To achieve this you define:\n",
    "\n",
    "- An __error measure__ and (classification error rate or precision and recall)\n",
    "\n",
    "- A __validation strategy__ \n",
    "\n",
    "Many validation strategies exist, including splitting and testing the data on:\n",
    "\n",
    "- A simple train and test split\n",
    "\n",
    "- K-folds cross validation \n",
    "\n",
    "- Leave-1 out validation \n",
    "\n",
    "__Train and test split__ trains the model on one training set and validates the model on one test test.\n",
    "\n",
    "![title](traintest.jpg)\n",
    "\n",
    "__K-folds cross validation__ divides the data set into k parts and uses each part one time as a test set while using the others as a training data set. This has the advantage that you use all the data available in the data set.\n",
    "\n",
    "![title](kfold.jpg)\n",
    "\n",
    "__Leave-1 out validation__ is the same as k-folds, but with k equal to the total number of examples. You always leave one observation out and train on the rest of the data. This is usually used only on small data sets, so it's more valuable to people evaulating laboratory experiments than to big data analysts.\n",
    "\n",
    "![title](leave1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Presenting/Reporting Findings\n",
    "\n",
    "Once the modelling and data analysis is complete, the findings need to be presented to the stakeholders. \n",
    "\n",
    "Appropriate visual tools can enhance results presentation:\n",
    "\n",
    "- Texts\n",
    "\n",
    "- Tables\n",
    "\n",
    "- Graphs\n",
    "\n",
    "Overall it's important to tell a compelling story (a formal report).\n",
    "\n",
    "### Automation\n",
    "\n",
    "Sometimes, people want to __repeat__ your work over and over again, because they value the predictions of your models or the insights.\n",
    "\n",
    "This __doesn't__ mean that:\n",
    "\n",
    "- you have to __redo__ all of your analysis all the time\n",
    "\n",
    "- sometimes it's sufficient that you implement only the __model scoring__\n",
    "\n",
    "- other times you might build an application that __automatically updates__ reports or spread sheets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
